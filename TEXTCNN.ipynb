{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>[국민청원 데이터 분류하기]</h1>\n",
    "\n",
    "<p>• 주목받을 만한 청원 분류하기</p>\n",
    "<p>• '주목 받는다'는 것은 주관적인 일이기 때문에 주관적 판단을 배제하기 위해 딥러닝을 도입 </p>\n",
    "<p>• 높은 청원 참여인원을 기록한 글들의 특징을 학습하여, 새로운 글이 입력되었을 때 청원 참여인원이 높은 글들과의 유사성을 계산하여 판단</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>[크롤링]</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sleep 90seconds. Count:584280, Local Time:2021-10-2016:15:49, Data Length: 7\n",
      "Sleep 90seconds. Count:585000, Local Time:2021-10-2016:18:37, Data Length: 73\n",
      "Sleep 90seconds. Count:585960, Local Time:2021-10-2016:21:54, Data Length: 120\n",
      "Sleep 90seconds. Count:586740, Local Time:2021-10-2016:24:50, Data Length: 166\n",
      "Sleep 90seconds. Count:594120, Local Time:2021-10-2016:39:14, Data Length: 258\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time \n",
    "\n",
    "result = pd.DataFrame()\n",
    "for i in range(584274, 595226):\n",
    "    URL = \"http://www1.president.go.kr/petitions/\"+str(i)\n",
    "    \n",
    "    response = requests.get(URL)\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    title = soup.find('h3', class_ = 'petitionsView_title')\n",
    "    count = soup.find('span', class_ = 'counter')\n",
    "    \n",
    "    for content in soup.select('div.petitionsView_write > div.View_write'):\n",
    "        content \n",
    "        a = []\n",
    "        for tag in soup.select('ul.petitionsView_info_list > li'):\n",
    "            a.append(tag.contents[1])\n",
    "            \n",
    "        if len(a) != 0:\n",
    "            df1 = pd.DataFrame({'start' : [a[1]],\n",
    "                                'end' : [a[2]],\n",
    "                                'category' : [a[0]],\n",
    "                                'count' : [count.text],\n",
    "                                'title' : [title.text],\n",
    "                                'content' : [content.text.strip()[0: 13000]]\n",
    "                              \n",
    "                               })\n",
    "            result = pd.concat([result, df1])\n",
    "            result.index = np.arange(len(result))\n",
    "            \n",
    "        if i % 60 == 0:\n",
    "            print(\"Sleep 90seconds. Count:\"+ str(i)\n",
    "                +\", Local Time:\"\n",
    "                + time.strftime('%Y-%m-%d', time.localtime(time.time()))\n",
    "                +\"\"+ time.strftime('%X', time.localtime(time.time()))\n",
    "                +\", Data Length: \" + str(len(result)))\n",
    "            time.sleep(90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>[크롤링 데이터 확인]</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(259, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>category</th>\n",
       "      <th>count</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>인권/성평등</td>\n",
       "      <td>267</td>\n",
       "      <td>서울지방병무청 탈의실에 설치된 CCTV에 대한 진상규명을 요구한다. 또한 인권위의 ...</td>\n",
       "      <td>본인은 2019년 8월 경 서울지방병무청 제1검사장 탈의실에서 믿을 수 없는 것을 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>경제민주화</td>\n",
       "      <td>271</td>\n",
       "      <td>주식시장 활성화 및 소액(개미)투자자 보호</td>\n",
       "      <td>우리 나라 코스피 시총이 미국 애플보다 작다는 설이 돌 정도로 한국의 주식시장은 저...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>행정</td>\n",
       "      <td>198</td>\n",
       "      <td>교정기관의 민낮</td>\n",
       "      <td>억울한 일로 국민청원을 신청합니다.\\n\\r\\n 저는 **구치소 **교도관입니다.\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>안전/환경</td>\n",
       "      <td>170</td>\n",
       "      <td>미세먼지 저감 대책</td>\n",
       "      <td>미세먼지의 심각성은 이제 적극적인 대안을 요구 하고 있습니다.\\r\\n우리 일상에서 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>교통/건축/국토</td>\n",
       "      <td>2,127</td>\n",
       "      <td>악질세입자 방지를 위한 세입자보호법을 재정해주세요.</td>\n",
       "      <td>저는 우선 아이셋의 부모입니다.\\r\\n식구가 많은편이고 아이들이 성장함에 따라 집이...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        start         end  category  count  \\\n",
       "0  2020-01-02  2020-02-01    인권/성평등    267   \n",
       "1  2020-01-02  2020-02-01     경제민주화    271   \n",
       "2  2020-01-02  2020-02-01        행정    198   \n",
       "3  2020-01-02  2020-02-01     안전/환경    170   \n",
       "4  2020-01-02  2020-02-01  교통/건축/국토  2,127   \n",
       "\n",
       "                                               title  \\\n",
       "0  서울지방병무청 탈의실에 설치된 CCTV에 대한 진상규명을 요구한다. 또한 인권위의 ...   \n",
       "1                            주식시장 활성화 및 소액(개미)투자자 보호   \n",
       "2                                           교정기관의 민낮   \n",
       "3                                         미세먼지 저감 대책   \n",
       "4                       악질세입자 방지를 위한 세입자보호법을 재정해주세요.   \n",
       "\n",
       "                                             content  \n",
       "0  본인은 2019년 8월 경 서울지방병무청 제1검사장 탈의실에서 믿을 수 없는 것을 ...  \n",
       "1  우리 나라 코스피 시총이 미국 애플보다 작다는 설이 돌 정도로 한국의 주식시장은 저...  \n",
       "2  억울한 일로 국민청원을 신청합니다.\\n\\r\\n 저는 **구치소 **교도관입니다.\\n...  \n",
       "3  미세먼지의 심각성은 이제 적극적인 대안을 요구 하고 있습니다.\\r\\n우리 일상에서 ...  \n",
       "4  저는 우선 아이셋의 부모입니다.\\r\\n식구가 많은편이고 아이들이 성장함에 따라 집이...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(result.shape)\n",
    "\n",
    "df = result\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>[크롤링 직후의 데이터]</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'우리 나라 코스피 시총이 미국 애플보다 작다는 설이 돌 정도로 한국의 주식시장은 저평가된 시장이라고 합니다. 하지만 투자매력이 없다고도 합니다.이렇게 말하는 이유가 어디 있습니까? 바로 투자를 해도 수익을 기대하기 어렵다는 인식이 이미 널리 퍼져 있다는 것입니다.\\n\\r\\n지금은 투자매력이 없어서 그렇지,..우리 나라 시중 부동 자금이 어마어마 한 것으로 알려진 것과, 외국 투자 자본 또한 실로 어마무지하게 많다는 것도 알고있습니다. 그러나 이 투자금이 주식시장으로 원활하게 순환이 안되고 있다는데 있습니다.\\n\\r\\n정부는 유휴자금이 주식 시장으로 들어오게 분위기를 띄어줘야 하는데,... 그렇지 못한 현실이 안타깝다고 생각합니다.\\r\\n국가가 지원해 돈 들이기가 어려우면,정부에서 정서적인 말이라도 활성화를 위한 관심표명과 정책적으로 지원만 해도 시장분위기는 많이 좋아질 것이라 확신합니다. \\n\\r\\n그래서 저는 정부에 다음과 같이 청원합니다.\\r\\n주식시장에 더 큰 충격 오기 전에 정부가 예방 조치를 할 수 있는데까지 노력해 주시기를 당부 드리면서,...\\n\\r\\n첫째,주식시장 활성화와 부양에 대한 정부의 의지를 표명해 주십시오 \\n\\r\\n둘째,  코스닥 종목은 공매도 제외시켜 주세요.\\n\\r\\n공매도 제도를 아예 없애버리면 좋겠지만 제도를 살린다면 적용 시장에서 코스닥 종목은 제외 해 주어야 체력이 약한 코스닥 시장을 안정시킬 수 있다고 봅니다 \\n\\r\\n셋째, 거래시장의 주식거래세를 더 낮춰 주세요 \\n\\r\\n특히 개미(소액)투자에 동일한 요율로 부과하는 것은 너무나 불공정 하다고 봅니다.\\n\\r\\n넷째, 중,장기 보유자에 대한 우대 차원에서 보유기간을 설정하여 주세요.\\n\\r\\n이는 장기 보유자에 대한 혜택을 주어 초단타매매 등 시장 질서를 교란하는 투자자와 분리하는 제도를 도입해 시장 안정화에 기여해 주시길 간곡히 부탁드리면서 청원합니다.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[1]['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>[전처리]</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_white_space(text):\n",
    "    text = re.sub(r'[\\t\\r\\n\\f\\v]','',str(text))\n",
    "    return text\n",
    "def remove_special_char(text):\n",
    "    text = re.sub('[^ ㄱ-ㅣ가-힣 0-9]+','',str(text))\n",
    "    return text\n",
    "df.title = df.title.apply(remove_white_space)\n",
    "df.title = df.title.apply(remove_special_char)\n",
    "\n",
    "df.content = df.content.apply(remove_white_space)\n",
    "df.content = df.content.apply(remove_special_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>[전처리되었는지 확인]</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'우리 나라 코스피 시총이 미국 애플보다 작다는 설이 돌 정도로 한국의 주식시장은 저평가된 시장이라고 합니다 하지만 투자매력이 없다고도 합니다이렇게 말하는 이유가 어디 있습니까 바로 투자를 해도 수익을 기대하기 어렵다는 인식이 이미 널리 퍼져 있다는 것입니다지금은 투자매력이 없어서 그렇지우리 나라 시중 부동 자금이 어마어마 한 것으로 알려진 것과 외국 투자 자본 또한 실로 어마무지하게 많다는 것도 알고있습니다 그러나 이 투자금이 주식시장으로 원활하게 순환이 안되고 있다는데 있습니다정부는 유휴자금이 주식 시장으로 들어오게 분위기를 띄어줘야 하는데 그렇지 못한 현실이 안타깝다고 생각합니다국가가 지원해 돈 들이기가 어려우면정부에서 정서적인 말이라도 활성화를 위한 관심표명과 정책적으로 지원만 해도 시장분위기는 많이 좋아질 것이라 확신합니다 그래서 저는 정부에 다음과 같이 청원합니다주식시장에 더 큰 충격 오기 전에 정부가 예방 조치를 할 수 있는데까지 노력해 주시기를 당부 드리면서첫째주식시장 활성화와 부양에 대한 정부의 의지를 표명해 주십시오 둘째  코스닥 종목은 공매도 제외시켜 주세요공매도 제도를 아예 없애버리면 좋겠지만 제도를 살린다면 적용 시장에서 코스닥 종목은 제외 해 주어야 체력이 약한 코스닥 시장을 안정시킬 수 있다고 봅니다 셋째 거래시장의 주식거래세를 더 낮춰 주세요 특히 개미소액투자에 동일한 요율로 부과하는 것은 너무나 불공정 하다고 봅니다넷째 중장기 보유자에 대한 우대 차원에서 보유기간을 설정하여 주세요이는 장기 보유자에 대한 혜택을 주어 초단타매매 등 시장 질서를 교란하는 투자자와 분리하는 제도를 도입해 시장 안정화에 기여해 주시길 간곡히 부탁드리면서 청원합니다'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[1]['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>[토크나이징]</h2>\n",
    "<p>• 형태소 분석기 패키지인 Konlpy모듈 import</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "\n",
    "df['title_token'] = df.title.apply(okt.morphs)\n",
    "df['content_token'] = df.content.apply(okt.nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>[파생변수 생성]</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start            object\n",
      "end              object\n",
      "category         object\n",
      "count             int64\n",
      "title            object\n",
      "content          object\n",
      "title_token      object\n",
      "content_token    object\n",
      "token_final      object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "df['token_final'] = df.title_token + df.content_token\n",
    "\n",
    "df['count'] = df['count'].replace({',' : ''}, regex = True).apply(lambda x : int(x))\n",
    "print(df.dtypes)\n",
    "df['label'] = df['count'].apply(lambda x: 'Yes' if x>=1000 else 'NO')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>[분석에 필요한 token_final과 label만 추출하여 df_drop에 저장]</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drop = df[['token_final','label']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>[단어 임베딩을 위한 Word2Vec import]</h3>\n",
    "<p>• WordVec은 단어의 의미와 유사도를 반영하여 단어를 벡터로 표현하는 방식</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=10730, vector_size=100, alpha=0.025)\n",
      "[('사태', 0.9909855723381042), ('로', 0.9900429844856262), ('19', 0.9897022843360901), ('대통령', 0.988412618637085), ('바이러스', 0.9878028035163879), ('지금', 0.9872011542320251), ('현재', 0.987085223197937), ('국가', 0.9869998693466187), ('위기', 0.9869287014007568), ('위', 0.9866728186607361)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "embedding_model = Word2Vec(df_drop['token_final'],\n",
    "                           sg = 1,\n",
    "                           vector_size = 100,\n",
    "                           window = 2,\n",
    "                           min_count = 1,\n",
    "                           workers = 4\n",
    "                           )\n",
    "print(embedding_model)\n",
    "\n",
    "model_result = embedding_model.wv.most_similar(\"코로나\")\n",
    "print(model_result) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>[임베딩 모델 저장 및 로드]</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('사태', 0.9909855723381042), ('로', 0.9900429844856262), ('19', 0.9897022843360901), ('대통령', 0.988412618637085), ('바이러스', 0.9878028035163879), ('지금', 0.9872011542320251), ('현재', 0.987085223197937), ('국가', 0.9869998693466187), ('위기', 0.9869287014007568), ('위', 0.9866728186607361)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "embedding_model.wv.save_word2vec_format('data\\petitions_tokens_w2v')\n",
    "\n",
    "loaded_model = KeyedVectors.load_word2vec_format\n",
    "(\"data\\petitions_tokens_w2v\")\n",
    "#model_result = loaded_model.most_similar(\"코로나\")\n",
    "print(model_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>[데이터를 Train, Validation set으로 랜덤하게 분할후 분할결과 저장]</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import RandomState\n",
    "\n",
    "rng = RandomState()\n",
    "\n",
    "tr = df_drop.sample(frac = 0.8, random_state=rng)\n",
    "val = df_drop.loc[~df_drop.index.isin(tr.index)]\n",
    "\n",
    "tr.to_csv('data/train.csv', index=False, encoding='utf-8-sig')\n",
    "val.to_csv('data/validation.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>[Field 클래스 정의]</h3>\n",
    "<p>• 자연어 처리 라이브러리인 torchtext 임포트</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.legacy.data import Field\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('[\\[\\]\\']','',str(text))\n",
    "    text = text.split(',')\n",
    "    return text\n",
    "TEXT = Field(tokenize=tokenizer)\n",
    "LABEL = Field(sequential=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>[데이터 불러오기]</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: ['검찰', ' 행태', ' 를', ' 개탄', ' 하며', ' 국민', ' 여러분', ' 들', ' 의', ' 도움', ' 을', ' 요청', ' 합니다', ' 저희', ' 아버지', ' 중국', ' 정부', ' 중국', ' 사업', ' 중국', ' 사업', ' 인맥', ' 친분', ' 온', ' 중국인', ' 친구', ' 지금', ' 인맥', ' 친분', ' 유지', ' 오시', ' 저희', ' 아버지', ' 고교', ' 동창', ' 친구', ' 자기', ' 친', ' 동생', ' 핫도그', ' 대표이사', ' 중국', ' 쪽', ' 핫도그', ' 중국', ' 지점', ' 개설', ' 고생', ' 중국', ' 쪽', ' 인맥', ' 친분', ' 저희', ' 아버지', ' 부탁', ' 저희', ' 아버지', ' 핫도그', ' 대표이사', ' 만남', ' 회의', ' 중국', ' 직접', ' 인맥', ' 동원', ' 핫도그', ' 대한', ' 업설', ' 중국', ' 친구', ' 그', ' 중국', ' 친구', ' 상당', ' 가능성', ' 사업', ' 생각', ' 연락', ' 사업', ' 진행', ' 과정', ' 핫도그', ' 판권', ' 저희', ' 아버지', ' 주기', ' 본격', ' 사업', ' 진행', ' 저희', ' 아버지', ' 제빵', ' 기술', ' 교육', ' 모로', ' 노력', ' 중국', ' 인맥', ' 통해', ' 핫도그', ' 중국', ' 지점', ' 사업', ' 추진', ' 성사', ' 사업', ' 중간', ' 아버지', ' 소개', ' 중국인', ' 투자자', ' 핫도그', ' 측', ' 뒤', ' 모종', ' 거래', ' 저희', ' 아버지', ' 사건', ' 서울중앙지법', ' 고소', ' 와중', ' 사건', ' 지법', ' 관', ' 수사', ' 날', ' 경찰서', ' 저', ' 아버지', ' 연락', ' 조서', ' 저희', ' 아버지', ' 조서', ' 경찰서', ' 조서', ' 중', ' 핫도그', ' 과', ' 저희', ' 아버지', ' 양해각서', ' 라이센스', ' 사용권', ' 등', ' 체결', ' 서류', ' 등', ' 직인', ' 자기', ' 저희', ' 아버지', ' 경찰관', ' 분', ' 핫도그', ' 이나', ' 저희', ' 아버지', ' 중', ' 거짓말', ' 사문서', ' 위조', ' 대질', ' 심문', ' 얘기', ' 그', ' 경찰관', ' 대답', ' 조만간', ' 대질', ' 심문', ' 말', ' 정도', ' 시간', ' 저희', ' 아버지', ' 사건', ' 담당', ' 경찰관', ' 분', ' 전화', ' 문의', ' 검사', ' 불기소', ' 사건', ' 종결', ' 얘기', ' 서로', ' 주장', ' 와중', ' 대질', ' 심문', ' 안', ' 불기소', ' 처분', ' 처음', ' 사건', ' 서울중앙지법', ' 지법', ' 관', ' 항소', ' 및', ' 재정신청', ' 권력', ' 법', ' 무시', ' 저희', ' 아버지', ' 국민', ' 청원', ' 사건', ' 제대로', ' 수사', ' 그냥', ' 명백', ' 서로', ' 주장', ' 사건', ' 무마', ' 게', ' 다시', ' 재', ' 수사', ' 저희', ' 아버지', ' 저희', ' 아버지', ' 이', ' 과정', ' 사건', ' 터', ' 무니', ' 종결', ' 항소', ' 및', ' 재정신청', ' 우리나라', ' 법', ' 현실', ' 개탄', ' 매일', ' 술', ' 우울증', ' 정신과', ' 치료', ' 정말', ' 다시', ' 재', ' 수사', ' 여러분', ' 도움', ' 좀'] NO\n",
      "Validation: ['악질', ' 세', ' 입자', ' 방지', ' 를', ' 위', ' 한', ' 세', ' 입자', ' 보호', ' 법', ' 을', ' 재정', ' 해주세요', ' 저', ' 우선', ' 아이', ' 셋', ' 부모', ' 식구', ' 편이', ' 아이', ' 집', ' 나름', ' 꿈', ' 가지', ' 좀더', ' 집', ' 분양', ' 아이', ' 학교', ' 문제', ' 분양', ' 집', ' 바로', ' 상황', ' 우선', ' 월세', ' 보증금', ' 백', ' 월', ' 보증금', ' 가격', ' 이유', ' 입자', ' 입자', ' 이후', ' 관리', ' 비', ' 한번', ' 걸', ' 알', ' 나름', ' 유주', ' 입자', ' 생활', ' 연락', ' 집주인', ' 관리', ' 비', ' 일일이', ' 체크', ' 웃기', ' 생각', ' 체납', ' 유주', ' 입자', ' 주시', ' 등', ' 한번', ' 주의', ' 알림', ' 관리소', ' 원망', ' 관리', ' 비', ' 보증금', ' 월세', ' 보증금', ' 멸후', ' 개월', ' 손해', ' 육박', ' 나름', ' 배려', ' 저희', ' 일주일', ' 뒤', ' 달뒤', ' 말', ' 전혀', ' 노력', ' 입자', ' 달', ' 달이', ' 지옥', ' 저희', ' 대출', ' 이자', ' 매', ' 마이너스', ' 통장', ' 이제', ' 더', ' 이상', ' 한계', ' 입자', ' 농락', ' 기분', ' 막상', ' 단전', ' 입자', ' 관리', ' 사무소', ' 입자', ' 보호', ' 법', ' 이야기', ' 이', ' 말', ' 사람', ' 난방', ' 수도', ' 전기', ' 저희', ' 빚', ' 왜', ' 국가', ' 입자', ' 보호', ' 법', ' 는걸', ' 국가', ' 돈', ' 보호', ' 유주', ' 돈', ' 세입', ' 자만', ' 보호', ' 유주', ' 악한', ' 유주', ' 단전', ' 집주인', ' 누가', ' 부동산', ' 입자', ' 보호', ' 법', ' 수정', ' 금지', ' 입자', ' 보호', ' 법', ' 보증금', ' 상태', ' 적용', ' 세입', ' 보호', ' 법도', ' 예외', ' 법안', ' 명도', ' 소송도', ' 보증금', ' 경우', ' 제외', ' 보증금', ' 입자', ' 농간', ' 소유', ' 주의', ' 빚', ' 때', ' 소송', ' 진행', ' 법안', ' 시기', ' 정말', ' 법', ' 악용', ' 악질', ' 입자', ' 존재', ' 정말', ' 유주', ' 입장', ' 악질', ' 입자', ' 때문', ' 가슴앓이', ' 눈물', ' 경우', ' 사실', ' 꼭', ' 인지', ' 내', ' 집', ' 내', ' 살', ' 보지', ' 못', ' 새집', ' 생판', ' 피', ' 타인', ' 점거', ' 어이', ' 제때', ' 가지', ' 못', ' 이', ' 뭐', ' 상황', ' 한탄'] Yes\n"
     ]
    }
   ],
   "source": [
    "from torchtext.legacy.data import TabularDataset\n",
    "\n",
    "train, validation = TabularDataset.splits(\n",
    "    path = 'data/',\n",
    "    train = 'train.csv',\n",
    "    validation = 'validation.csv',\n",
    "    format = 'csv',\n",
    "    fields = [('text', TEXT),('label', LABEL)],\n",
    "    skip_header = True\n",
    ")\n",
    "\n",
    "print(\"Train:\", train[0].text, train[0].label)\n",
    "print(\"Validation:\", validation[0].text,validation[0].label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>[단어장 및 DataLoader 정의]</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임베딩 벡터의 개수와 차원:torch.Size([9836, 100])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchtext.vocab import Vectors\n",
    "from torchtext.legacy.data import BucketIterator\n",
    "\n",
    "vectors = Vectors(name=\"data/petitions_tokens_w2v\")\n",
    "\n",
    "TEXT.build_vocab(train,\n",
    "                 vectors = vectors,min_freq = 1,max_size = None)\n",
    "LABEL.build_vocab(train)\n",
    "\n",
    "vocab = TEXT.vocab\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_iter, validation_iter = BucketIterator.splits(\n",
    "    datasets = (train,validation),\n",
    "    batch_size = 8,\n",
    "    device = device,\n",
    "    sort = False\n",
    ")\n",
    "print('임베딩 벡터의 개수와 차원:{}'.format(TEXT.vocab.vectors.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>TextCNN 모델 설계</h2>\n",
    "<p>• 문장을 단어 기준으로 나누어 컴퓨터가 단어를 이해할 수 있도록 텍스트를 벡터 형태로 전환하는 임베딩 실시</p>\n",
    "<p>• TextCNN은 문장의 문맥적 의미를 파악하는 과정에서 정보를 집약해주기 때문에 연산이 빠르기때문에 사용</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_built, emb_dim, dim_channel, kernel_wins, num_class):\n",
    "        \n",
    "        super(TextCNN, self).__init__()\n",
    "        \n",
    "        self.embed = nn.Embedding(len(vocab_built), emb_dim)\n",
    "        self.embed.weight.data.copy_(vocab_built.vectors)\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, dim_channel, (w, emb_dim)) for w in kernel_wins])\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.fc = nn.Linear(len(kernel_wins)*dim_channel, num_class)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "            \n",
    "        emb_x = self.embed(x)\n",
    "        emb_x = emb_x.unsqueeze(1)\n",
    "            \n",
    "        con_x = [self.relu(conv(emb_x)) for conv in self.convs]\n",
    "            \n",
    "        pool_x = [F.max_pool1d(x.squeeze(-1), x.size()[2]) for x in con_x]\n",
    "        fc_x = torch.cat(pool_x, dim=1)\n",
    "        fc_x = fc_x.squeeze(-1)\n",
    "        fc_x = self.dropout(fc_x)\n",
    "        logit = self.fc(fc_x)\n",
    "            \n",
    "        return logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>[모델 학습 함수 정의]</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_itr, optimizer):\n",
    "    \n",
    "    model.train()\n",
    "    corrects, train_loss = 0.0,0\n",
    "    \n",
    "    for batch in train_itr:\n",
    "        \n",
    "        text, target = batch.text, batch.label\n",
    "        text = torch.transpose(text, 0, 1)\n",
    "        target.data.sub_(1)\n",
    "        text, target = text.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logit = model(text)\n",
    "        \n",
    "        loss = F.cross_entropy(logit, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        result = torch.max(logit,1)[1]\n",
    "        corrects += (result.view(target.size()).data == target.data).sum()\n",
    "    \n",
    "    train_loss /= len(train_itr.dataset)\n",
    "    accuracy = 100.0 * corrects / len(train_itr.dataset)\n",
    "    \n",
    "    return train_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>[모델 평가 함수 정의]</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate (model, device, itr):\n",
    "    \n",
    "    model.eval()\n",
    "    corrects, test_loss = 0.0,0\n",
    "    \n",
    "    for batch in itr:\n",
    "        \n",
    "        text = batch.text\n",
    "        target = batch.label\n",
    "        text = torch.transpose(text, 0, 1)\n",
    "        target.data.sub_(1)\n",
    "        text, target = text.to(device), target.to(device)\n",
    "        \n",
    "        logit = model(text)\n",
    "        loss = F.cross_entropy(logit, target)\n",
    "        \n",
    "        test_loss += loss.item()\n",
    "        result = torch.max(logit,1)[1]\n",
    "        corrects += (result.view(target.size()).data == target.data).sum()\n",
    "        \n",
    "    test_loss /= len(itr.dataset)\n",
    "    accuracy = 100.0 * corrects / len(itr.dataset)\n",
    "    \n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>[모델 학습 및 성능 확인]</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextCNN(\n",
      "  (embed): Embedding(9836, 100)\n",
      "  (convs): ModuleList(\n",
      "    (0): Conv2d(1, 10, kernel_size=(3, 100), stride=(1, 1))\n",
      "    (1): Conv2d(1, 10, kernel_size=(4, 100), stride=(1, 1))\n",
      "    (2): Conv2d(1, 10, kernel_size=(5, 100), stride=(1, 1))\n",
      "  )\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      "  (fc): Linear(in_features=30, out_features=2, bias=True)\n",
      ")\n",
      "Train Epoch: 1 \t Loss: 0.08575502513111502 \t Accuracy: 56.5217399597168%\n",
      "Valid Epoch: 1 \t Loss: 0.08886105166031764 \t Accuracy: 63.46154022216797%\n",
      "model saves at 63.46154022216797 accuracy\n",
      "-----------------------------------------------------------------------------\n",
      "Train Epoch: 2 \t Loss: 0.08684974131376846 \t Accuracy: 56.03864669799805%\n",
      "Valid Epoch: 2 \t Loss: 0.09033933511147133 \t Accuracy: 51.92307662963867%\n",
      "-----------------------------------------------------------------------------\n",
      "Train Epoch: 3 \t Loss: 0.0841432455657185 \t Accuracy: 58.454105377197266%\n",
      "Valid Epoch: 3 \t Loss: 0.08899582234712747 \t Accuracy: 65.38461303710938%\n",
      "model saves at 65.38461303710938 accuracy\n",
      "-----------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = TextCNN(vocab, 100, 10,[3,4,5],2).to(device)\n",
    "print(model)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "best_test_acc = -1\n",
    "\n",
    "for epoch in range(1,3+1):\n",
    "    \n",
    "    tr_loss, tr_acc = train(model, device, train_iter, optimizer) \n",
    "    \n",
    "    print('Train Epoch: {} \\t Loss: {} \\t Accuracy: {}%'.format(epoch, tr_loss, tr_acc))\n",
    "    \n",
    "    val_loss, val_acc = evaluate(model, device, validation_iter)\n",
    "    \n",
    "    print('Valid Epoch: {} \\t Loss: {} \\t Accuracy: {}%'.format(epoch, val_loss, val_acc))\n",
    "    \n",
    "    if val_acc > best_test_acc:\n",
    "        best_test_acc = val_acc\n",
    "        \n",
    "        print(\"model saves at {} accuracy\".format(best_test_acc))\n",
    "        torch.save(model.state_dict(), \"TextCNN_Best_Validation\")\n",
    "    \n",
    "    print('-----------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
